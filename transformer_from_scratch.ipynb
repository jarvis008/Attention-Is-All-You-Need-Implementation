{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        self.head_weights = []\n",
    "        assert embed_size == self.head_dim * heads, \"embedding size should be divisible by head dimension\"\n",
    "\n",
    "        for head in range(heads):\n",
    "            self.query_weights = nn.Linear(self.embed_size, self.head_dim, bias=False)\n",
    "            self.key_weights = nn.Linear(self.embed_size, self.head_dim, bias=False)\n",
    "            self.value_weights = nn.Linear(self.embed_size, self.head_dim, bias=False)\n",
    "            self.head_weights.append([self.query_weights, self.key_weights, self.value_weights])\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        '''\n",
    "        query_shape ---> (N, query_len, embed_size)\n",
    "        key_shape ---> (N, key_len, embed_size)\n",
    "        value_shape ---> (N, value_len, embed_size)\n",
    "        query_len, key_len, value_len ---> number of tokens in a given sample sentence\n",
    "        '''\n",
    "        N = query.shape[0]\n",
    "        q_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        val_len = value.shape[1]\n",
    "        Attention_Heads = torch.zeros([N, q_len, self.head_dim, self.heads], dtype=torch.float64)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            query_keyT_dot_product_by_sqrt_dk = torch.dot(self.head_weights[head][0](query), self.head_weights[head][1](key).T)/math.sqrt(self.head_dim)\n",
    "            if mask is not None:\n",
    "                query_keyT_dot_product_by_sqrt_dk = query_keyT_dot_product_by_sqrt_dk.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            Attention_Heads[:, :, :, head] = torch.matmul(softmax(query_keyT_dot_product_by_sqrt_dk), self.head_weights[head][2](value))\n",
    "        Attention_heads = Attention_Heads.view(-1, q_len, self.heads*self.head_dim)\n",
    "        MultiHead_Attention = self.fc_out(Attention_heads)\n",
    "        \n",
    "        return MultiHead_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_and_norm_multihead(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(add_and_norm_multihead, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.layer_norm_multihead = nn.LayerNorm(self.embed_size)\n",
    "\n",
    "    def forward(self, multihead, prev_query):\n",
    "        return self.layer_norm_multihead(torch.add(multihead, prev_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_and_norm_feedforward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(add_and_norm_feedforward, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.layer_norm_feedforward = nn.LayerNorm(self.embed_size)\n",
    "\n",
    "    def forward(self, feedforward_output, prev_output):\n",
    "        return self.layer_norm_feedforward(torch.add(feedforward_output, prev_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforward(nn.Module):\n",
    "    def __init__(self, embed_size, forward_expansion, dropout):\n",
    "        super(feedforward, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, math.floor(embed_size * forward_expansion)), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(math.floor(forward_expansion*embed_size), embed_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, normalized_multihead):\n",
    "        x = self.dropout(normalized_multihead)\n",
    "        out = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, embed_size, forward_expansion, dropout, heads, mask=None):\n",
    "        super(encoder_block, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attention = SelfAttention(embed_size, heads)\n",
    "        self.add_and_norm_multihead = add_and_norm_multihead(embed_size)\n",
    "        self.add_and_norm_feedforward = add_and_norm_feedforward(embed_size)\n",
    "        self.feed_forward = feedforward(embed_size, forward_expansion, dropout)\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        multihead = self.self_attention.forward(query, key, value, self.mask)\n",
    "        normalized_multihead = self.add_and_norm_multihead.forward(multihead, query)\n",
    "        feed_forward_output = self.feed_forward.forward(normalized_multihead)\n",
    "        normalized_feed_forward_output = self.add_and_norm_feedforward.forward(feed_forward_output, normalized_multihead)\n",
    "        normalized_feedforward_output_after_dropout = self.dropout(normalized_feed_forward_output)\n",
    "        return normalized_feedforward_output_after_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, embed_size, forward_expansion, dropout, heads, encoding, mask):\n",
    "        super(decoder_block, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attention_1 = SelfAttention(embed_size, heads)\n",
    "        self.self_attention_2 = SelfAttention(embed_size, heads)\n",
    "        self.add_and_norm_multihead = add_and_norm_multihead(embed_size)\n",
    "        self.add_and_norm_feedforward = add_and_norm_feedforward(embed_size)\n",
    "        self.feed_forward = feedforward(embed_size, forward_expansion, dropout)\n",
    "        self.mask = mask\n",
    "        self.encoding = encoding\n",
    "        self.key_weights = nn.Linear(embed_size, embed_size)\n",
    "        self.value_weights = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        multihead_1 = self.self_attention_1.forward(query, key, value, self.mask)\n",
    "        normalized_multihead_1 = self.add_and_norm_multihead.forward(multihead_1, query)\n",
    "        key_2 = self.key_weights(self.encoding)\n",
    "        value_2 = self.value_weights(self.encoding)\n",
    "        multihead_2 = self.self_attention_2.forward(normalized_multihead_1, key_2, value_2, mask=None)\n",
    "        normalized_multihead_2 = self.add_and_norm_multihead.forward(multihead_2, normalized_multihead_1)\n",
    "        feed_forward_output = self.feed_forward.forward(normalized_multihead_2)\n",
    "        normalized_feed_forward_output = self.add_and_norm_feedforward.forward(feed_forward_output, normalized_multihead_2)\n",
    "        normalized_feedforward_output_after_dropout = self.dropout(normalized_feed_forward_output)\n",
    "        return normalized_feedforward_output_after_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, embedding, layers=6, heads=8, forward_expansion=1.6, dropout=0.4):\n",
    "        # embedding shape ---> (batch_size, max_len, embed_size)\n",
    "        super(encoder, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.embed_size = embedding.shape[2]\n",
    "        self.layers = layers\n",
    "        self.heads = heads\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.dropout = dropout\n",
    "        self.enc_blocks = nn.ModuleList([encoder_block(self.embed_size, self.forward_expansion, self.dropout, self.heads) for i in range(self.layers)])\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.enc_blocks[0](self.embedding)\n",
    "        for i in range(1, self.layers):\n",
    "            x = self.enc_blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, input, encoding, mask, layers=6, heads=8, forward_expansion=1.6, dropout=0.4):\n",
    "        # encoding shape ---> (batch_size, max_len, embed_size)\n",
    "        # input shape ---> (batch_size, max_len, embed_size) {if n words are generated, in case of NLG, (max_len - n) words are masked}\n",
    "        super(encoder, self).__init__()\n",
    "        self.input = input\n",
    "        self.encoding = encoding\n",
    "        self.embed_size = encoding.shape[2]\n",
    "        self.layers = layers\n",
    "        self.heads = heads\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.dropout = dropout\n",
    "        self.dec_blocks = nn.ModuleList([decoder_block(self.embed_size, forward_expansion, dropout, heads, encoding, mask) for i in range(self.layers)])\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.dec_blocks[0](self.input)\n",
    "        for i in range(1, self.layers):\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_classification(nn.Module):\n",
    "    def __init__(self, input, encoding, layers=6, heads=8, forward_expansion=1.6, dropout=0.4):\n",
    "        # encoding shape ---> (batch_size, max_len, embed_size)\n",
    "        # input shape ---> (batch_size, max_len, embed_size) {if n words are generated, in case of NLG, (max_len - n) words are masked}\n",
    "        super(encoder, self).__init__()\n",
    "        self.input = input\n",
    "        self.encoding = encoding\n",
    "        self.embed_size = encoding.shape[2]\n",
    "        self.layers = layers\n",
    "        self.heads = heads\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.dropout = dropout\n",
    "        self.dec_blocks = nn.ModuleList([decoder_block(self.embed_size, forward_expansion, dropout, heads, encoding) for i in range(self.layers)])\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.dec_blocks[0](self.input)\n",
    "        for i in range(1, self.layers):\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, input_embedding_batch, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cf554c516b0411fd1632c9677ac3071bc6327432929dd2350bde96333029bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
